<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>
<div id="document" class="RNA  skn-hdr1 fontsize fontface vmargins hmargins pagesize dynamicbg">
    <div id="SECTION_SUMM88a9a60c-8d24-40ab-9d64-0b22aef26317" class="section">
        <div class="heading">
            <div class="sectiontitle" id="SECTNAME_SUMM88a9a60c-8d24-40ab-9d64-0b22aef26317">Summary</div>
        </div>
        <div class="paragraph firstparagraph"
             id="PARAGRAPH_88a9a60c-8d24-40ab-9d64-0b22aef26317_1_f9df753e-c48e-48ed-b4b2-31cc71b98fd7">
            <div class="field singlecolumn" id="88a9a60c-8d24-40ab-9d64-0b22aef26317FRFM1">
                <ul>
                    <li>Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer
                        with expertise in designing data intensive applications using Hadoop Ecosystem , Big Data
                        Analytical , Cloud Data engineering , Data Warehouse / Data Mart, Data Visualization , Reporting
                        , and Data Quality solutions .
                    </li>
                    <li>Experienced in migrating database/legacy applications to <strong>AWS</strong> cloud ecosystem using Services like <strong>VPC, EC2, S3, EMR, RDS</strong> for Compute, <strong>Big Data</strong> Analytics and Storage.</li>
                    <li>Worked on multiple projects and possess strong skills in Application Enhancements, Performance Tuning and Production Support and Highly experienced in converting data from legacy systems and flat files.</li>
                    <li>Worked extensively in SQL (ANSI and Postgres) and expert in Trouble shooting the PL/SQL code and experienced in advanced features like Collection objects, Bulk collection, Dynamic SQL, Ref Cursor, XMLDB features, Autonomous Transactions, Partitions, Global temp tables, Pipeline and Table functions, Cursor expressions and various types of Indexes.</li>
                    <li>Spined up an Amazon EMR cluster to run sqoop jobs to migrate data from on premise oracle database to S3. Used S3 as source and read data through Athena for data analytics. Tested this process using different file formats such as CSV, Parquet and AVRO. Partitioned and compressed data to optimize and to save cost.</li>
                    <li>In - depth knowledge of Hadoop architecture and its components like YARN , HDFS, Name Node, Data
                        Node, Job Tracker, Application Master, Resource Manager , Task Tracker and Map Reduce
                        programming paradigm.
                    </li>
                    <li>Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop
                        components such as Apache Spark, MapReduce, HDFS, Sqoop, PIG, Hive, HBase, Oozie, Flume, NiFi,
                        Kafka, Zookeeper, and YARN.
                    </li>
                    <li>Profound experience in performing Data Ingestion, Data Processing (Transformations, enrichment,
                        and aggregations).
                    </li>
                    <li>Strong Knowledge on Architecture of Distributed systems and Parallel processing, In-depth
                        understanding of MapReduce programming paradigm and Spark execution framework.
                    </li>
                    <li>Experienced with the Spark improving the performance and optimization of the existing algorithms
                        in Hadoop using Spark Context , Spark-SQL , Dataframe API , Spark Streaming, MLlib , Pair RDD 's
                        and worked explicitly on PySpark and Scala .
                    </li>
                    <li>Handled ingestion of data from different data sources into HDFS using Sqoop, Flume and perform
                        transformations using Hive, Map Reduce and then loading data into HDFS.
                    </li>
                    <li>Managed Sqoop jobs with incremental load to populate HIVE external tables. Experience in
                        importing streaming data into HDFS using Flume sources, and Flume sinks and transforming the
                        data using Flume interceptors.
                    </li>
                    <li>Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph ( DAG )
                        of actions with control flows.
                    </li>
                    <li>Implemented the security requirements for Hadoop and integrating with Kerberos authentication
                        infrastructure- KDC server setup, creating realm /domain, managing.
                    </li>
                    <li>Experience of Partitions, bucketing concepts in Hive and designed both Managed and External
                        tables in Hive to optimize performance .
                    </li>
                    <li>Experience with different file formats like Avro , parquet , ORC , Json and XML .</li>
                    <li>Expertise in Creating, Debugging, Scheduling and Monitoring jobs using Airflow and Oozie.</li>
                    <li>Experienced with using most common Operators in Airflow - Python Operator, Bash Operator, Google
                        Cloud Storage Download Operator, Google Cloud Storage Object Sensor.
                    </li>
                    <li>Hands-on experience in handling database issues and connections with SQL and NoSQL databases
                        such as MongoDB , HBase , Cassandra , SQL server , and PostgreSQL .
                    </li>
                    <li>Created Java apps to handle data in MongoDB and HBase. Used Phoenix to create SQL layer on
                        HBase.
                    </li>
                    <li>Experience in designing and creating RDBMS Tables, Views, User Created Data Types, Indexes,
                        Stored Procedures, Cursors, Triggers and Transactions.
                    </li>
                    <li>Expert in designing ETL data flows using creating mappings/workflows to extract data from SQL
                        Server and Data Migration and Transformation from Oracle/Access/Excel Sheets using SQL Server
                        SSIS .
                    </li>
                    <li>Expert in designing Parallel jobs using various stages like Join, Merge, Lookup, remove
                        duplicates, Filter, Dataset, Lookup file set, Complex flat file, Modify, Aggregator, XML.
                    </li>
                    <li>Hands-on experience with Amazon EC2, Amazon S3, Amazon RDS, VPC, IAM, Amazon Elastic Load
                        Balancing, Auto Scaling, CloudWatch, SNS, SES, SQS, Lambda, EMR and other services of the AWS
                        family.
                    </li>
                    <li>Created and configured new batch job in Denodo scheduler with email notification capabilities
                        and Implemented Cluster setting for multiple Denodo node and created load balance for improving
                        performance activity.
                    </li>
                    <li>Instantiated, created, and maintained CI/CD (continuous integration &amp; deployment) pipelines
                        and apply automation to environments and applications.
                    </li>
                    <li>Worked on various automation tools like GIT, Terraform, Ansible. Experienced in fact dimensional
                        modeling ( Star schema, Snowflake schema ), transactional modeling and SCD (Slowly changing
                        dimension)
                    </li>
                    <li>Experienced with JSON based RESTful web services, and XML/QML based SOAP web services and also
                        worked on various applications using python integrated IDEs like Sublime Text and PyCharm .
                    </li>
                    <li>Efficient Cloud Engineer with years of experience assembling cloud infrastructure. Utilizes
                        strong managerial skills by negotiating with vendors and coordinating tasks with other IT team
                        members. Implements best practices to create cloud functions, applications and databases.
                    </li>
                </ul>
            </div>
        </div>
    </div>
    <div id="SECTION_HILT45780143-a42f-4da3-bf8e-f55f7a869f70" class="section">
        <div class="heading">
            <div class="sectiontitle" id="SECTNAME_HILT45780143-a42f-4da3-bf8e-f55f7a869f70">Skills</div>
        </div>
        <div class="paragraph firstparagraph"
             id="PARAGRAPH_45780143-a42f-4da3-bf8e-f55f7a869f70_1_bbd5f917-4d70-9f52-5ac3-b996562ac704">
            <div class="singlecolumn maincolumn">
                <table class="twocol skill">
                    <tbody>
                    <tr>
                        <td class="field twocol_1" id="45780143-a42f-4da3-bf8e-f55f7a869f70SKC10"><p>· <strong>Big Data
                            Technologies: </strong>Hadoop, MapReduce, HDFS, Sqoop, PIG, Hive, HBase, Oozie, Flume, NiFi,
                            Kafka, Zookeeper, Yarn, Apache Spark, Mahout, Sparklib</p>
                            <p>· <strong>Databases: </strong>Oracle, MySQL, SQL Server, MongoDB, Cassandra, DynamoDB,
                                PostgreSQL, Teradata, Cosmos.</p>
                            <p>· <strong>Programming: </strong>Python, PySpark, Scala, Java, C, C++, Shell script, Perl
                                script, SQL</p>
                            <p>· <strong>Cloud Technologies: </strong>AWS, Microsoft Azure</p>
                            <p>· <strong>Frameworks: </strong>Django REST framework, MVC, Hortonworks</p>
                            <p>· <strong>Tools:</strong> PyCharm, Eclipse, Visual Studio, SQL*Plus, SQL Developer, TOAD,
                                SQL Navigator, Query Analyzer, SQL Server Management Studio, SQL Assistance, Eclipse,
                                Postman</p>
                            <p>· <strong>Versioning tools: </strong>SVN, Git, GitHub</p></td>
                        <td class="field twocol_2" id="45780143-a42f-4da3-bf8e-f55f7a869f70SKC20"><p>· <strong>Operating
                            Systems: </strong>Windows 7/8/XP/2008/2012, Ubuntu Linux, MacOS</p>
                            <p>· <strong>Network Security: </strong>Kerberos</p>
                            <p>· <strong>Database Modelling: </strong>Dimension Modeling, ER Modeling, Star Schema
                                Modeling, Snowflake Modeling</p>
                            <p>· <strong>Monitoring Tool: </strong>Apache Airflow</p>
                            <p>· <strong>Visualization/ Reporting: </strong>Tableau, ggplot2, matplotlib, SSRS and Power
                                BI</p>
                            <p>· <strong>Machine Learning Techniques: </strong>Linear &amp; Logistic Regression,
                                Classification and Regression Trees, Random Forest, Associative rules, NLP and
                                Clustering.</p></td>
                    </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </div>
    <div id="SECTION_EXPR37766849-10fe-47ef-b314-620ff3a82a0f" class="section">
        <div class="heading">
            <div class="sectiontitle" id="SECTNAME_EXPR37766849-10fe-47ef-b314-620ff3a82a0f">Experience</div>
        </div>
        <div class="paragraph firstparagraph"
             id="PARAGRAPH_37766849-10fe-47ef-b314-620ff3a82a0f_1_f77225c7-d7c1-4375-ad17-6cef97255898" itemscope=""
             itemtype="https://schema.org/Organization">
            <div class="singlecolumn"><span class="paddedline padb5 txtRglr" itemscope=""
                                            itemtype="https://schema.org/postalAddress"><span class="jobtitle"
                                                                                              id="37766849-10fe-47ef-b314-620ff3a82a0fJTIT0">AWS Data Engineer</span><span
                    class="sprtr"><span> / </span></span><span class="companyname"
                                                               id="37766849-10fe-47ef-b314-620ff3a82a0fCOMP0"
                                                               itemprop="name">Accenture Contractor Jobs</span><span><span> - </span></span><span
                    class="jobcity" id="37766849-10fe-47ef-b314-620ff3a82a0fJCIT0"
                    itemprop="addressLocality">Rochester </span><span>, </span><span class="jobstate"
                                                                                     id="37766849-10fe-47ef-b314-620ff3a82a0fJSTA0"
                                                                                     itemprop="addressRegion">  NY</span><span
                    class="joblocation jobcountry" id="37766849-10fe-47ef-b314-620ff3a82a0fJCNT0"></span><span
                    id="37766849-10fe-47ef-b314-620ff3a82a0fJCTR0"></span><span class="datesWrapper txtItl"><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fJSTD0" format="%m/%Y">01/2022</span><span> - </span><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fEDDT0" format="%m/%Y">02/2022</span></span></span><span
                    class="jobline" id="37766849-10fe-47ef-b314-620ff3a82a0fJDES0" itemprop="description"><ul><li>Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics, processing, storing and Reporting of voluminous, rapidly changing data</li><li>Responsible for maintaining quality reference data in source by performing operations such as cleaning, transformation and ensuring Integrity in a relational environment by working closely with the stakeholders &amp; solution architect</li><li>Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda, DynamoDB</li><li>Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS, Hive, Pig and MapReduce to access cluster for new users</li><li>Performed end- to-end Architecture &amp; implementation assessment of various AWS services like Amazon EMR, Redshift, S3</li><li>Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake</li><li>Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases, such as Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB</li><li>Used Spark SQL for Scala &amp; amp, Python interface that automatically converts RDD case classes to schema RDD</li><li>Import the data from different sources like HDFS/HBase into Spark RDD and perform computations using PySpark to generate the output response</li><li>Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources</li><li>Importing &amp; exporting database using SQL Server Integrations Services (SSIS) and Data Transformation Services (DTS Packages)</li><li>Coded Teradata BTEQ scripts to load, transform data, fix defects like SCD 2 date chaining, cleaning up duplicates</li><li>Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects</li><li>Conducted Data blending, Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server</li><li>Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions</li><li>Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3, training ML model and deploying it for prediction</li><li>Integrated Apache Airflow with AWS to monitor multi-stage ML workflows with the tasks running on Amazon SageMaker</li><li>Environment: AWS EMR, S3, RDS, Redshift, Lambda, Boto3, DynamoDB, Amazon SageMaker, Apache Spark, HBase, Apache Kafka, HIVE, SQOOP, Map Reduce, Snowflake, Apache Pig, Python, SSRS, Tableau</li><li>Assessed organization technology infrastructure and managed cloud migration process.</li><li>Configured computing, networking and security systems within cloud environment.</li><li>Implemented cloud policies, managed technology requests and maintained service availability.</li></ul></span>
            </div>
        </div>
        <div class="paragraph"
             id="PARAGRAPH_37766849-10fe-47ef-b314-620ff3a82a0f_2_cd723dc8-dd07-4d0c-a857-b0ec295bf865" itemscope=""
             itemtype="https://schema.org/Organization">
            <div class="singlecolumn"><span class="paddedline padb5 txtRglr" itemscope=""
                                            itemtype="https://schema.org/postalAddress"><span class="jobtitle"
                                                                                              id="37766849-10fe-47ef-b314-620ff3a82a0fJTIT1">Data Engineer</span><span
                    class="sprtr"><span> / </span></span><span class="companyname"
                                                               id="37766849-10fe-47ef-b314-620ff3a82a0fCOMP1"
                                                               itemprop="name">Verizon</span><span><span> - </span></span><span
                    class="jobcity" id="37766849-10fe-47ef-b314-620ff3a82a0fJCIT1"
                    itemprop="addressLocality">Beaverton </span><span>, </span><span class="jobstate"
                                                                                     id="37766849-10fe-47ef-b314-620ff3a82a0fJSTA1"
                                                                                     itemprop="addressRegion">  OR</span><span
                    class="joblocation jobcountry" id="37766849-10fe-47ef-b314-620ff3a82a0fJCNT1"></span><span
                    id="37766849-10fe-47ef-b314-620ff3a82a0fJCTR1"></span><span class="datesWrapper txtItl"><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fJSTD1" format="%m/%Y">01/2016</span><span> - </span><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fEDDT1" format="%m/%Y">11/2019</span></span></span><span
                    class="jobline" id="37766849-10fe-47ef-b314-620ff3a82a0fJDES1" itemprop="description"><ul><li>Worked on Azure Data Factory to integrate data of both on-prem (MY SQL, Cassandra) and cloud (Blob storage, Azure SQL DB) and applied transformations to load back to Azure Synapse</li><li>Managed, Configured and scheduled resources across the cluster using Azure Kubernetes Service</li><li>Monitored Spark cluster using Log Analytics and Ambari Web UI</li><li>Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance</li><li>Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL</li><li>Also Worked with Cosmos DB (SQL API and Mongo API)</li><li>Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services (SSRS) and Power BI</li><li>Performed the migration of large data sets to Databricks (Spark), create and administer cluster, load data, configure data pipelines, loading data from ADLS Gen2 to Databricks using ADF pipelines</li><li>Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB</li><li>Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick</li><li>Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps, ADF pipelines, and other services like HTTP requests, email triggers etc</li><li>Worked extensively on Azure data factory including data transformations, Integration Runtimes, Azure Key Vaults, Triggers and migrating data factory pipelines to higher environments using ARM Templates</li><li>Ingested data in mini-batches and performs RDD transformations on those mini-batches of data by using Spark Streaming to perform streaming analytics in Data bricks</li><li>Environment: Azure SQL DW, Databrick, Azure Synapse, Cosmos DB, ADF, SSRS, Power BI, Azure Data lake, ARM, Azure HDInsight, Blob storage, Apache Spark.</li><li>Adept in troubleshooting and identifying current issues and providing effective solutions.</li><li>Managed performance monitoring and tuning while identifying and repairing issues within database realm.</li><li>Identified key use cases and associated reference architectures for market segments and industry verticals.</li><li>Designed surveys, opinion polls, and assessment tools to collect data.</li><li>Tested, validated and reformulated models to foster accurate prediction of outcomes.</li><li>Created graphs and charts detailing data analysis results.</li><li>Recommended data analysis tools to address business issues.</li><li>Developed new functions and applications to conduct analyses.</li><li>Cleaned and manipulated raw data.</li><li>Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts.</li></ul></span>
            </div>
        </div>
        <div class="paragraph"
             id="PARAGRAPH_37766849-10fe-47ef-b314-620ff3a82a0f_3_831fff74-a9e5-443e-9d9c-ca9deb9984e3" itemscope=""
             itemtype="https://schema.org/Organization">
            <div class="singlecolumn"><span class="paddedline padb5 txtRglr" itemscope=""
                                            itemtype="https://schema.org/postalAddress"><span class="jobtitle"
                                                                                              id="37766849-10fe-47ef-b314-620ff3a82a0fJTIT2">Big Data Engineer / Hadoop Developer</span><span
                    class="sprtr"><span> / </span></span><span class="companyname"
                                                               id="37766849-10fe-47ef-b314-620ff3a82a0fCOMP2"
                                                               itemprop="name">Two95 International Inc.</span><span><span> - </span></span><span
                    class="jobcity" id="37766849-10fe-47ef-b314-620ff3a82a0fJCIT2"
                    itemprop="addressLocality">Boca Raton </span><span>, </span><span class="jobstate"
                                                                                      id="37766849-10fe-47ef-b314-620ff3a82a0fJSTA2"
                                                                                      itemprop="addressRegion">  FL</span><span
                    class="joblocation jobcountry" id="37766849-10fe-47ef-b314-620ff3a82a0fJCNT2"></span><span
                    id="37766849-10fe-47ef-b314-620ff3a82a0fJCTR2"></span><span class="datesWrapper txtItl"><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fJSTD2" format="%m/%Y">10/2013</span><span> - </span><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fEDDT2" format="%m/%Y">12/2015</span></span></span><span
                    class="jobline" id="37766849-10fe-47ef-b314-620ff3a82a0fJDES2" itemprop="description">AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi<ul><li>Interacted with business partners, Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem</li><li>Developed Spark Streaming programs to process near real time data from Kafka, and process data with both stateless and state full transformations</li><li>Worked with HIVE data warehouse infrastructure-creating tables, data distribution by implementing partitioning and bucketing, writing and optimizing the HQL queries</li><li>Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60% of execution time</li><li>Worked on developing ETL processes (Data Stage Open Studio) to load data from multiple data sources to HDFS using FLUME and SQOOP, and performed structural modifications using Map Reduce, HIVE</li><li>D eveloping Spark scripts, UDFS using both Spark DSL and Spark SQL query for data aggregation, querying, and writing data back into RDBMS through Sqoop</li><li>Written multiple MapReduce Jobs using Java API, Pig and Hive for data extraction, transformation and aggregationAvrom multiple file formats including Parquet, Avro, XML, JSON, CSV, ORCFILE and other compressed file formats Codecs like gZip, Snappy, Lzo</li><li>Strong understanding of Partitioning, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance</li><li>Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders</li><li>Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake</li><li>Experience in report writing using SQL Server Reporting Services (SSRS) and creating various types of reports like drill down, Parameterized, Cascading, Conditional, Table, Matrix, Chart and Sub Reports</li><li>Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database</li><li>Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs</li><li>Worked on implementation of a log producer in Scala that watches for application logs, transform incremental log and sends them to a Kafka and Zookeeper based log collection platform</li><li>Used Hive to analyze data ingested into HBase by using Hive-HBase integration and compute various metrics for reporting on the dashboard</li><li>Transformed tPySpark using AWS Glue dynamic frames with PySpark; cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature</li><li>Worked on installing cluster, commissioning &amp; decommissioning of data node, name node recovery, capacity planning, and slots configuration</li><li>Developed data pipeline programs with Spark Scala APIs, data aggregations with Hive, and formatting data (JSON) for visualization, and generatiPySpark
  </li><li>Environment: AWS, Cassandra, PySpark, Apache Spark, HBase, Apache Kafka, HIVE, SQOOP, FLUME, Apache oozie, Zookeeper, ETL, UDF, Map Reduce, Snowflake, Apache Pig, Python, Java, SSRS</li><li>Onfidential</li><li>Developed and implemented Hadoop code while observing coding standards.</li><li>Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds.</li><li>Developed new functions and applications to conduct analyses.</li><li>Created graphs and charts detailing data analysis results.</li><li>Tested, validated and reformulated models to foster accurate prediction of outcomes.</li></ul></span>
            </div>
        </div>
        <div class="paragraph"
             id="PARAGRAPH_37766849-10fe-47ef-b314-620ff3a82a0f_4_d6b4e056-aa19-4596-8900-116d1d0c0f7b" itemscope=""
             itemtype="https://schema.org/Organization">
            <div class="singlecolumn"><span class="paddedline padb5 txtRglr" itemscope=""
                                            itemtype="https://schema.org/postalAddress"><span class="jobtitle"
                                                                                              id="37766849-10fe-47ef-b314-620ff3a82a0fJTIT3">Python Developer </span><span
                    class="sprtr"><span> / </span></span><span class="companyname"
                                                               id="37766849-10fe-47ef-b314-620ff3a82a0fCOMP3"
                                                               itemprop="name">Fiserv</span><span><span> - </span></span><span
                    class="jobcity" id="37766849-10fe-47ef-b314-620ff3a82a0fJCIT3"
                    itemprop="addressLocality">City </span><span>, </span><span class="jobstate"
                                                                                id="37766849-10fe-47ef-b314-620ff3a82a0fJSTA3"
                                                                                itemprop="addressRegion">  STATE</span><span
                    class="joblocation jobcountry" id="37766849-10fe-47ef-b314-620ff3a82a0fJCNT3"></span><span
                    id="37766849-10fe-47ef-b314-620ff3a82a0fJCTR3"></span><span class="datesWrapper txtItl"><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fJSTD3" format="%m/%Y">09/2012</span><span> - </span><span
                    class="jobdates" id="37766849-10fe-47ef-b314-620ff3a82a0fEDDT3" format="%m/%Y">10/2013</span></span></span><span
                    class="jobline" id="37766849-10fe-47ef-b314-620ff3a82a0fJDES3" itemprop="description"><ul><li>AWS, S3, EC2, LAMBDA, EBS, IAM, Datadog, CloudTrail, CLI, Ansible, MySQL, Python, Git, Jenkins, DynamoDB, Cloud Watch, Docker, Kubernetes</li><li>Leveraged open communication, collective decision-making and thorough reviews to create performant and scalable systems.</li><li>Worked with server-side and front-end technologies and leveraged common design patterns to code dynamic and user-friendly systems.</li><li>Implemented new API routes, architected new ORM structures and refactored code to boost application performance.</li><li>Harnessed version control tools to coordinate project development and individual code submissions.</li><li>Introduced cloud-based technologies into Python development to expand on-premise deployment options.</li><li>Wrote clear and clean code for use in projects.</li><li>Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes.</li></ul></span>
            </div>
        </div>
    </div>
    <div id="SECTION_EDUC31c1d9a3-4525-4133-abd8-2a2189cbaf29" class="section">
        <div class="heading">
            <div class="sectiontitle" id="SECTNAME_EDUC31c1d9a3-4525-4133-abd8-2a2189cbaf29">Education and Training
            </div>
        </div>
        <div class="paragraph firstparagraph"
             id="PARAGRAPH_31c1d9a3-4525-4133-abd8-2a2189cbaf29_1_2bfc01e7-515b-31fa-4023-2e97a9f84c7b" itemscope=""
             itemtype="https://schema.org/EducationalOrganization">
            <div class="singlecolumn"><span class="paddedline" itemprop="description"><span class="paddedline"><span
                    class="degree" id="31c1d9a3-4525-4133-abd8-2a2189cbaf29DGRE0"
                    itemprop="name">Post Graduate </span><span><span><span
                    class="beforecolonspace"></span><span>: </span></span></span><span class="programline"
                                                                                       id="31c1d9a3-4525-4133-abd8-2a2189cbaf29STUY0">Data Engineering </span><span></span><span
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29GRHN0"></span><span class="datesWrapper txtItl"><span
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29GRYR0" format="%m/%Y">02/2022</span></span></span><span
                    class="paddedline"><span class="companyname companyname_educ"
                                             id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCHO0" itemprop="name">Purdue University</span><span
                    class="hyphen"> - </span><span class="joblocation jobcity"
                                                   id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCIT0">City</span><span>, </span><span
                    class="joblocation jobstate" id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SSTA0">State</span><span
                    class="joblocation jobcountry"
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCNT0"></span></span></span><span class="field"
                                                                                              id="31c1d9a3-4525-4133-abd8-2a2189cbaf29FRFM0"></span>
            </div>
        </div>
        <div class="paragraph"
             id="PARAGRAPH_31c1d9a3-4525-4133-abd8-2a2189cbaf29_2_6cf4a00f-08b4-c5bf-db4b-59bee764e796" itemscope=""
             itemtype="https://schema.org/EducationalOrganization">
            <div class="singlecolumn"><span class="paddedline" itemprop="description"><span class="paddedline"><span
                    class="degree" id="31c1d9a3-4525-4133-abd8-2a2189cbaf29DGRE1"
                    itemprop="name">Post Graduate </span><span><span><span
                    class="beforecolonspace"></span><span>: </span></span></span><span class="programline"
                                                                                       id="31c1d9a3-4525-4133-abd8-2a2189cbaf29STUY1">Data Science And Business Analytics </span><span></span><span
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29GRHN1"></span><span class="datesWrapper txtItl"><span
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29GRYR1" format="%m/%Y">09/2021</span></span></span><span
                    class="paddedline"><span class="companyname companyname_educ"
                                             id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCHO1" itemprop="name">University of Texas At Austin</span><span
                    class="hyphen"> - </span><span class="joblocation jobcity"
                                                   id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCIT1">City</span><span>, </span><span
                    class="joblocation jobstate" id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SSTA1">State</span><span
                    class="joblocation jobcountry"
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCNT1"></span></span></span><span class="field"
                                                                                              id="31c1d9a3-4525-4133-abd8-2a2189cbaf29FRFM1"></span>
            </div>
        </div>
        <div class="paragraph"
             id="PARAGRAPH_31c1d9a3-4525-4133-abd8-2a2189cbaf29_3_0b506566-eb83-1ae1-3e84-83e7dfdc8d0c" itemscope=""
             itemtype="https://schema.org/EducationalOrganization">
            <div class="singlecolumn"><span class="paddedline" itemprop="description"><span class="paddedline"><span
                    class="degree" id="31c1d9a3-4525-4133-abd8-2a2189cbaf29DGRE2"
                    itemprop="name">Bachelor of Arts</span><span><span><span
                    class="beforecolonspace"></span><span>: </span></span></span><span class="programline"
                                                                                       id="31c1d9a3-4525-4133-abd8-2a2189cbaf29STUY2">Business Administration And Management</span><span></span><span
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29GRHN2"></span><span class="datesWrapper txtItl"><span
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29GRYR2" format="%m/%Y">12/2009</span></span></span><span
                    class="paddedline"><span class="companyname companyname_educ"
                                             id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCHO2" itemprop="name">Califonia State University </span><span
                    class="hyphen"> - </span><span class="joblocation jobcity"
                                                   id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCIT2">City</span><span
                    class="joblocation jobstate" id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SSTA2"></span><span
                    class="joblocation jobcountry"
                    id="31c1d9a3-4525-4133-abd8-2a2189cbaf29SCNT2"></span></span></span><span class="field"
                                                                                              id="31c1d9a3-4525-4133-abd8-2a2189cbaf29FRFM2"></span>
            </div>
        </div>
    </div>
</div>
</body>
</html>